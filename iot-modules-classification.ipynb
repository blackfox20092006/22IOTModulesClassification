{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11589868,"sourceType":"datasetVersion","datasetId":7267425}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\n#path\nroot = '/kaggle/input/iot-modules-singleclass/iot-modules-singleclass'\ntrain, valid, test = root+'/train/', root+'/valid/', root+'/test/'","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-30T16:35:56.769567Z","iopub.execute_input":"2025-04-30T16:35:56.769816Z","iopub.status.idle":"2025-04-30T16:35:56.773303Z","shell.execute_reply.started":"2025-04-30T16:35:56.769799Z","shell.execute_reply":"2025-04-30T16:35:56.772675Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"#library imports\nimport torch\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchvision import models, transforms\nimport cv2, os\nimport matplotlib.pyplot as plt","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-30T16:35:56.774730Z","iopub.execute_input":"2025-04-30T16:35:56.775167Z","iopub.status.idle":"2025-04-30T16:36:01.477274Z","shell.execute_reply.started":"2025-04-30T16:35:56.775149Z","shell.execute_reply":"2025-04-30T16:36:01.476716Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"#test func padding\nimg = cv2.imread('/kaggle/input/iot-modules-singleclass/iot-modules-singleclass/test/20240610_124954_jpg.rf.87b57d00a4229f5a0438ee03914efea6.jpg')\nH, W, C = img.shape\nprint(H, W, C)\nT, B, L, R = 0, 0, 0, 0 # top bottom left right\nif H > W: # cao hơn rộng => padding chiều rộng\n    R = H-W\nelif H < W: #rộng hơn cao => padding chiều cao\n    T = W-H\nelse: #bang nhau\n    pass\nprint(R, T)\ntransform = transforms.Compose(\n    [\n        transforms.ToPILImage(),\n        transforms.Pad(padding=(L, T, R, B), fill=0, padding_mode='constant'), #trái - trên - phải - dưới\n        transforms.Resize((224, 224)),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n    ]\n)\n# a = transform(img)\n# print(a)\n# plt.imshow(a.permute(1, 2, 0))\n# plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-30T16:36:01.477876Z","iopub.execute_input":"2025-04-30T16:36:01.478132Z","iopub.status.idle":"2025-04-30T16:36:01.596809Z","shell.execute_reply.started":"2025-04-30T16:36:01.478117Z","shell.execute_reply":"2025-04-30T16:36:01.596067Z"}},"outputs":[{"name":"stdout","text":"4032 3024 3\n1008 0\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"def get_paths_and_1hotvectors(path):\n    file = open(path+'_classes.csv', 'r')\n    DATA = file.readlines()\n    label_truth = DATA[0].replace('\\n', '').split(',')[1:]\n    data = []\n    for i in DATA[1:]:\n        filename = path + i.split(',')[0]\n        onehot_vector = [int(j) for j in i.split(',')[1:]]\n        data.append([filename, onehot_vector])\n    return label_truth, data\n# x, y = get_paths_and_1hotvectors(test)\n# print(x[16])\n# print(x)\n# print(y[0][1].index(1))\n# print(y[0][0])\n# print(y[0][1])\n'''\nPhotoresistor\n['7-segment-LED', 'Board-Test-830', 'Buzzer', 'Dust-sensor-GP2Y1014AU-PM2.5', 'Fingerprint-sensor-AS608-XD-65', 'IR-Remote', 'IR-receiver-1838T', 'KIT-Arduino-UNO-R3-ATMEGA16U2', 'KIT-WiFi-NodeMcu-ESP8266-CH340', 'Keypad-4x4-SMD', 'LCD-1602', 'LED', 'Led-Matrix-8x8', 'Module-Bluetooth-HC05', 'Module-Lora-RF433-SX1278-RA-01', 'Module-Matrix-8x32-MAX7219-MT-832-G', 'Photoresistor', 'Potentiometer', 'Realtime-Module-DS1307', 'Temperature-LM35', 'Ultrasonic-Range-Finder-SRF05', 'servo-motor-SG90']\n16\n/kaggle/input/iot-modules-singleclass/iot-modules-singleclass/test/20240610_130149_jpg.rf.064f93788c00415dff972f5ed4c5db2b.jpg\n[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]\n'''\ntrain_ground_truth, train_img_paths_and_1hotvectors = get_paths_and_1hotvectors(train)\nvalid_ground_truth, valid_img_paths_and_1hotvectors = get_paths_and_1hotvectors(valid)\ntest_ground_truth, test_img_paths_and_1hotvectors = get_paths_and_1hotvectors(test)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-30T16:36:01.597646Z","iopub.execute_input":"2025-04-30T16:36:01.597925Z","iopub.status.idle":"2025-04-30T16:36:01.628031Z","shell.execute_reply.started":"2025-04-30T16:36:01.597901Z","shell.execute_reply":"2025-04-30T16:36:01.627561Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"class IOTDataset(Dataset):\n    def __init__(self, img_paths, labels, transforms=False): #label at one hot vectors\n        self.img_paths = img_paths\n        self.labels = labels\n        self.transforms = transforms\n    def __len__(self):\n        return len(self.img_paths)\n    def __getitem__(self, index):\n        path, onehotvector2intlabel = self.img_paths[index], self.labels[index].index(1)\n        #img\n        img_data = cv2.imread(path)\n        if transforms:\n            H, W, C = img_data.shape\n            # print(H, W, C)\n            T, B, L, R = 0, 0, 0, 0 # top bottom left right\n            if H > W: # cao hơn rộng => padding chiều rộng\n                R = H-W\n            elif H < W: #rộng hơn cao => padding chiều cao\n                T = W-H\n            else: #bang nhau\n                pass\n            # print(R, T)\n            transform = transforms.Compose(\n                [\n                    transforms.ToPILImage(),\n                    transforms.Pad(padding=(L, T, R, B), fill=0, padding_mode='constant'), #trái - trên - phải - dưới\n                    transforms.RandomRotation(degrees=5),\n                    transforms.RandomHorizontalFlip(p=0.5),\n                    transforms.ColorJitter(\n                        brightness=0.05,\n                        contrast=0.05,\n                        saturation=0.05,\n                        hue=0.02\n                    ),\n\n                    transforms.Resize((224, 224)),\n                    transforms.ToTensor(),\n                    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n                ]\n            )\n            img_data_tensor = transform(img_data)\n        #label\n        label_tensor = torch.tensor(onehotvector2intlabel, dtype=torch.long)\n        return img_data_tensor, label_tensor\n\n#dataset\ntrain_img_paths, train_label_vectors, valid_img_paths, valid_label_vectors, test_img_paths, test_label_vectors = [], [], [], [], [], []\nfor i in train_img_paths_and_1hotvectors:\n    train_img_paths += [i[0]]\n    train_label_vectors += [i[1]]\nfor i in valid_img_paths_and_1hotvectors:\n    valid_img_paths += [i[0]]\n    valid_label_vectors += [i[1]]\nfor i in test_img_paths_and_1hotvectors:\n    test_img_paths += [i[0]]\n    test_label_vectors += [i[1]]\ntrain_dataset = IOTDataset(train_img_paths, train_label_vectors, transforms=True)\nvalid_dataset = IOTDataset(valid_img_paths, valid_label_vectors, transforms=True)\ntest_dataset = IOTDataset(test_img_paths, test_label_vectors, transforms=True)\n\n#dataloader\ntrain_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\nvalid_dataloader = DataLoader(valid_dataset, batch_size=64, shuffle=True)\ntest_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=True)\n\n#model\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nmodel = models.resnet101(pretrained=True)\nfor param in model.parameters():\n    param.requirse_grad = True\nmodel.fc = torch.nn.Linear(model.fc.in_features, 22)\nmodel = model.to(device)\n\n#loss and optimizer\nloss_func = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n#training\nepochs = 50\nbest_accuracy = 0.0\nfor i in range(epochs):\n    print(f'Epoch {i} / {epochs}.')\n    model.train()\n    running_loss, correct, total = 0, 0, 0\n    for batch_data in train_dataloader:\n        img_data, label_data = batch_data\n        img_data, label_data = img_data.to(device), label_data.to(device)\n\n        optimizer.zero_grad()\n        outputs = model(img_data)\n        loss = loss_func(outputs, label_data)\n\n        predictions = outputs.argmax(dim=1)\n        accuracy = sum(predictions == label_data) / len(label_data)\n\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item()\n        correct += (predictions == label_data).sum().item()\n        total += label_data.size(0)\n    epoch_loss = running_loss / len(train_dataloader)\n    epoch_accuracy = correct / total\n    print(f\"Train Loss: {epoch_loss:.12f}, Accuracy: {epoch_accuracy:.12f}.\")\n\n    #validation\n    model.eval()\n    val_loss, val_correct, val_total = 0, 0, 0\n    with torch.no_grad():\n        for batch_data in valid_dataloader:\n            # img_data, label_data = batch_data\n            # img_data, label_data = img_data.to(device), label_data.to(device)\n            # outputs = model(img_data)\n            # loss = loss_func(outputs, label_data)\n    \n            # predictions += outputs.argmax(dim=1)\n            # val_correct += (predictions == label_data).sum().item()\n            # val_total += label_data.size(0)\n            # val_loss += loss.item()\n            img_data, label_data = batch_data\n            img_data, label_data = img_data.to(device), label_data.to(device)\n            outputs = model(img_data)\n            loss = loss_func(outputs, label_data)\n\n            predictions = outputs.argmax(dim=1)\n            val_correct += (predictions == label_data).sum().item()\n            val_total += label_data.size(0)\n            val_loss += loss.item()\n    \n        test_loss = val_loss / len(test_dataloader)\n        test_accuracy = val_correct / val_total\n        print(f\"Test Loss: {test_loss:.12f}, Test Accuracy: {test_accuracy:12f}\")\n    if test_accuracy > best_accuracy:\n        best_accuracy = test_accuracy\n        torch.save(model.state_dict(), f'best_model{i}.pth')\n        print(f\"Saved best model! Accuracy: {best_accuracy:.6f}\")\n\nprint(f'Best accuracy achieved = {best_accuracy}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-30T16:36:01.629200Z","iopub.execute_input":"2025-04-30T16:36:01.629427Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet101_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet101_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/resnet101-63fe2227.pth\" to /root/.cache/torch/hub/checkpoints/resnet101-63fe2227.pth\n100%|██████████| 171M/171M [00:00<00:00, 201MB/s]  \n","output_type":"stream"},{"name":"stdout","text":"Epoch 0 / 50.\n","output_type":"stream"}],"execution_count":null}]}